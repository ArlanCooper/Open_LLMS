大语言模型评价
===


# 测评方法
## GPT4做裁判
通过OpenAI的GPT-4来评分：GPT-4是目前大家公认的能力最强的大模型，因此，我们用GPT-4来评价各个模型的问答效果。我们采用以下格式来对GPT-4提问：

```angular2html
您是一个乐于助人和准确的助理，用于检查回复具体问题的答案的质量。
[问题]：{question}
[答案]：{answer}
我们希望您对某个AI助手在上述问题的回答表现提供反馈。你将根据从0到10的评分获得总体综合表现得分，其中较高的分数表示整体表现更好。请首先输出一行单独包含一个数值，表示助手的得分。接下来的一行，请提供您评价的全面解释，避免任何潜在的偏见。

```



## 人工评价
通过用户的反馈来评分：在每个问题的答案下方，我们展示了不同模型的回答，用户可以“点赞”或“点踩”每个答案。我们将统计每个模型的答案得分进行评价，这是一种最为客观和准确的评价方式，但是依赖于大量的用户反馈。


# 测评内容
根据我们对大量开源问答数据、社交媒体、论坛、书籍的整理（例如：BELLE、十万个为什么、弱智吧等），经过反复的讨论和梳理，最终设计了八个大的类别来探索大模型的能力，包括：通用知识、语言理解、创作能力、逻辑推理、代码编程、工作技能、使用工具、人格特征。在每个大的类别下又细分了不同的子类，以尽可能全面地覆盖大模型的能力范畴。







# 参考
1. https://github.com/AtomEcho/AtomBulb